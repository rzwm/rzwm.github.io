<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Posts on </title>
    <link>/post/</link>
    <description>Recent content in Posts on </description>
    <generator>Hugo -- gohugo.io</generator>
    <language>zh-CN</language>
    <lastBuildDate>Sun, 27 Oct 2019 00:00:00 +0000</lastBuildDate>
    
	<atom:link href="/post/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>C&#43;&#43;11高精度计时器类</title>
      <link>/post/2019/10/27/c-11%E9%AB%98%E7%B2%BE%E5%BA%A6%E8%AE%A1%E6%97%B6%E5%99%A8%E7%B1%BB/</link>
      <pubDate>Sun, 27 Oct 2019 00:00:00 +0000</pubDate>
      
      <guid>/post/2019/10/27/c-11%E9%AB%98%E7%B2%BE%E5%BA%A6%E8%AE%A1%E6%97%B6%E5%99%A8%E7%B1%BB/</guid>
      <description>做图像处理算法时，免不了要测量函数的运行时间。以前我都是使用OpenCV的计时函数cv::getTickCount()和cv::getTickFrequency()，但是这样一来，在不使用OpenCV的项目中就没法用了。
幸好C++11增加了std::chrono库，可以很方便地实现跨平台的时间测量。于是我封装了一个简单的计时器类，这样只要将其简单地保存为头文件，就可以直接包含使用了。
此类当前的计时单位为毫秒，但可以去除/ 1e3从而精确到微秒。
#include &amp;lt;iostream&amp;gt; #include &amp;lt;chrono&amp;gt; class Timer { public: Timer() : t1(res::zero()) , t2(res::zero()) { tic(); } ~Timer() {} void tic() { t1 = clock::now(); } void toc(const char* str) { t2 = clock::now(); std::cout &amp;lt;&amp;lt; str &amp;lt;&amp;lt; &amp;quot; time: &amp;quot; &amp;lt;&amp;lt; std::chrono::duration_cast&amp;lt;res&amp;gt;(t2 - t1).count() / 1e3 &amp;lt;&amp;lt; &amp;quot;ms.&amp;quot; &amp;lt;&amp;lt; std::endl; } private: typedef std::chrono::high_resolution_clock clock; typedef std::chrono::microseconds res; clock::time_point t1; clock::time_point t2; };  测试代码如下：
int main() { Timer timer; std::cout &amp;lt;&amp;lt; &amp;quot;1&amp;quot; &amp;lt;&amp;lt; std::endl; timer.</description>
    </item>
    
    <item>
      <title>使用Guided Filter去除彩色噪声</title>
      <link>/post/2018/07/19/%E4%BD%BF%E7%94%A8guided-filter%E5%8E%BB%E9%99%A4%E5%BD%A9%E8%89%B2%E5%99%AA%E5%A3%B0/</link>
      <pubDate>Thu, 19 Jul 2018 23:56:27 +0000</pubDate>
      
      <guid>/post/2018/07/19/%E4%BD%BF%E7%94%A8guided-filter%E5%8E%BB%E9%99%A4%E5%BD%A9%E8%89%B2%E5%99%AA%E5%A3%B0/</guid>
      <description>注：因图床原因，本文图片已挂
手机在暗光环境下拍照时，由于进光量不足，拍出的照片上经常会出现严重的彩色噪声，如下图所示： 这种彩色噪声极大地影响了图像的观感，所以必须去除。而要在手机上运行，算法也一定要轻量化。这里提出一种轻量级而且比较实用的去彩噪的方法：使用Guided Filter去噪。
算法的步骤如下：
 将需要去噪的RGB图像(记为I)转换为YUV格式，记为I_YUV； 对I进行guided filter，I自己作为引导图像，得到滤波后的图像，记为I_gf； 将I_gf转换为YUV格式，记为I_gf_YUV； 使用I_YUV的Y通道替换I_gf_YUV的Y通道，得到新的YUV图像，记为I_Y_gf_UV; 将I_Y_gf_UV转换为RGB格式，即为去噪后图像。  去噪后的效果如下： 可以看到，除了颜色有一点冲淡外，去彩噪的效果相当显著。
测试代码如下：
// Win10 + Visual Studio Community 2017 + OpenCV 3.3.0 #include &amp;quot;opencv2/highgui/highgui.hpp&amp;quot; #include &amp;quot;opencv2/imgproc/imgproc.hpp&amp;quot; #include &amp;quot;opencv2/ximgproc.hpp&amp;quot; #include &amp;lt;iostream&amp;gt; void chromaNoiseReduction(const cv::Mat&amp;amp; src, cv::Mat&amp;amp; dst, int radius = 30, double eps = 0.03 * 255 * 255) { CV_Assert(src.type() == CV_8UC3); CV_Assert(src.cols % 2 == 0 &amp;amp;&amp;amp; src.rows % 2 == 0); const cv::Mat&amp;amp; I = src; // step 1.</description>
    </item>
    
    <item>
      <title>基于快速去雾的图像亮度增强方法</title>
      <link>/post/2018/06/09/%E5%9F%BA%E4%BA%8E%E5%BF%AB%E9%80%9F%E5%8E%BB%E9%9B%BE%E7%9A%84%E5%9B%BE%E5%83%8F%E4%BA%AE%E5%BA%A6%E5%A2%9E%E5%BC%BA%E6%96%B9%E6%B3%95/</link>
      <pubDate>Sat, 09 Jun 2018 21:37:09 +0000</pubDate>
      
      <guid>/post/2018/06/09/%E5%9F%BA%E4%BA%8E%E5%BF%AB%E9%80%9F%E5%8E%BB%E9%9B%BE%E7%9A%84%E5%9B%BE%E5%83%8F%E4%BA%AE%E5%BA%A6%E5%A2%9E%E5%BC%BA%E6%96%B9%E6%B3%95/</guid>
      <description>起因 最近在做一个图像处理的算法IBEABFHR，因为图像太暗，所以需要对图像的亮度进行增强(不考虑噪声的放大)。尝试了网上搜索到的各种方法后，发现它们存在两个问题：容易造成原本较亮的地方过曝，并且参数不好设置。尝试了一些暗光增强的paper的算法后，发现它们又太慢了。这时我想到曾经看过的一篇paper说过，有一种亮度增强的算法是基于去雾来做的，步骤很简单：
 将RGB图像取反(关于图像取反，请参考我的这篇博客OpenCV图像取反的快速方法)；
 对取反后的图像进行去雾；
 将去雾后的图像取反。
  其背后的理念是：暗光图像取反后，原本接近黑色的像素就会变成接近白色，整张图像就会类似于有雾的图像。于是对这样的图像进行去雾后，白色的像素就变暗了，再反色后，像素就变亮了！没毛病！
快速去雾算法 说起去雾，恐怕大多数人的第一反应就是鼎鼎大名的何恺明博士的暗通道先验去雾算法。我的第一反应也是这个。但是在了解之后，我发现这种方法速度太慢了，难以应用到我的算法中。于是我开始搜索快速的去雾算法，很快找到了这篇论文《基于单幅图像的快速去雾算法》刘倩, 陈茂银, 周东华(这篇文章中也提到了暗通道先验去雾算法的速度太慢)，速度很快，只有O(1)复杂度。
这篇论文我并没有仔细看，而是秉承“拿来主义”的精神，直接根据论文提供的算法流程实现了代码。原因是它的算法流程太简单了，在轻松地实现了代码之后，就没有再看的欲望了~这里贴一下它的算法流程，你们自己看： 一种可实时处理 O(1)复杂度图像去雾算法的实现。
源码解释 我把实现的代码放到了GitHub：IBEABFHR(原谅我这个取名废)，请点进去看效果图，我这里就不重复放了。我感觉效果还是很好的，暗处的亮度增强得很好，亮处虽然有过曝，但并不是很明显。而且控制亮度的参数很好调整，只要随便找一张图像调整好参数，就可以应用于所有图片了。
运行速度 代码是用OpenCV实现的，同时支持彩色图像和灰度图像。在我的电脑上(CPU: E3-1230 v3)测试，运行100次取平均值，速度如下：
   分辨率 类型 时间     1024x768 灰度图像 8.77ms   1024x768 彩色图像 16.24ms   1920x1080 灰度图像 22.61ms   1920x1080 彩色图像 40.60ms   4160x2340 灰度图像 104.57ms   4160x2340 彩色图像 186.14ms    但是如果你只使用我的算法一次，可能速度要慢得多，原因是第一次取反操作因为未知原因耗费了额外的时间(猜测是冷启动所致)。
参数调整 这个算法总共有两个可变参数。一个是在step 3中进行均值滤波时的所用的滤波半径radius，另一个是在step 5中用的ρ。</description>
    </item>
    
    <item>
      <title>OpenCV两个Mat相减的隐藏秘密</title>
      <link>/post/2018/06/08/opencv%E4%B8%A4%E4%B8%AAmat%E7%9B%B8%E5%87%8F%E7%9A%84%E9%9A%90%E8%97%8F%E7%A7%98%E5%AF%86/</link>
      <pubDate>Fri, 08 Jun 2018 23:01:32 +0000</pubDate>
      
      <guid>/post/2018/06/08/opencv%E4%B8%A4%E4%B8%AAmat%E7%9B%B8%E5%87%8F%E7%9A%84%E9%9A%90%E8%97%8F%E7%A7%98%E5%AF%86/</guid>
      <description>起因 今天在看同事写的代码时，发现一个“错误”： 他的原意是实现以下功能：
cv::Mat absDiff; cv::absdiff(mat1, mat2, absDiff);  其中mat1和mat2均为CV_8UC1类型。
但是可能是一时没想起这个函数，于是他写成了这个样子：
cv::Mat absDiff = cv::abs(mat1 - mat2);  问题 于是我认真地告诉他，这样做是错的。假设mat1为[0]，mat2为[255]，那么mat1 - mat2将会得到[0]，因为cv::saturate_cast&amp;lt;uchar&amp;gt;(0 - 255) == 0。则cv::abs([0])自然就是[0]，而他的期望是得到[255]。
并且我写出如下代码证明他是错的：
cv::Mat diff = mat1 - mat;  结果diff确实是[0]。那么cv::Mat absDiff = cv::abs(diff);肯定就是[0]了。
但是他坚持让我用cv::Mat absDiff = cv::abs(mat1 - mat2);测试。结果。。。。absDiff竟然真的是[255]！我当时就震惊了，同时隐隐有一种感觉：这其中一定隐藏着一个天大的秘密。
秘密 于是我进入调试模式，认真观察每一步，终于明白了玄机所在。
为什么我得到了[0] 先来分析我的测试代码：
cv::Mat diff = mat1 - mat;  在我以前的观念里，mat1 - mat是两个cv::Mat互相作用，实际上调用的是cv::subtract()。但是事实上，mat1 - mat2调用的是以下函数：
CV_EXPORTS MatExpr operator - (const Mat&amp;amp; a, const Mat&amp;amp; b); MatExpr operator - (const Mat&amp;amp; a, const Mat&amp;amp; b) { MatExpr e; MatOp_AddEx::makeExpr(e, a, b, 1, -1); return e; }  可以看到，mat1 - mat2实际上是生成了一个MatExpr对象。在生成过程中，并没有进行实际的相减操作，而只是保存了a和b，并记录了它们之间期望进行的操作：相减。实际的相减操作是在类型转换时进行的：</description>
    </item>
    
    <item>
      <title>OpenCL优化小技巧：预创建所有Kernel</title>
      <link>/post/2018/06/07/opencl%E4%BC%98%E5%8C%96%E5%B0%8F%E6%8A%80%E5%B7%A7%E9%A2%84%E5%88%9B%E5%BB%BA%E6%89%80%E6%9C%89kernel/</link>
      <pubDate>Thu, 07 Jun 2018 23:48:43 +0000</pubDate>
      
      <guid>/post/2018/06/07/opencl%E4%BC%98%E5%8C%96%E5%B0%8F%E6%8A%80%E5%B7%A7%E9%A2%84%E5%88%9B%E5%BB%BA%E6%89%80%E6%9C%89kernel/</guid>
      <description>最近做了一些图像处理的算法，跑在高通移动平台上，其中使用了OpenCL进行加速。在此过程中，也总结了几个加速的小技巧。今天就来谈其中一个不太有用的小技巧：预创建所有Kernel。
第一次进行OpenCL加速时，我注意到，创建cl_kernel时，会耗费几毫秒到二十几毫秒的时间。如果算法中需要创建几十个cl_kernel，那花费的时间也有几百毫秒了。这让人很难接受。
后来我又注意到，对于同一个Kernel，只有第一次创建时才会花费那么多时间，后续再次创建(无论前面创建的是否已经释放掉)所花费的时间将会大大减少，几乎可以忽略不计。那么机会就来了：我们可以在初始化时预先创建好所有Kernel，再全部释放掉。然后在实际的处理算法中，和平时一样使用clCreateKernel()和clReleaseKernel()，同时也不用担心创建Kernel所花费的额外时间了。
为什么我说这个小技巧不太有用呢？因为只有使用源码，也就是clCreateProgramWithSource()创建program时，才会出现创建Kernel耗时严重的现象。而一般发布出来的算法都会使用二进制文件，也就是clCreateProgramWithBinary()，在保存为二进制时，一定已经先创建所有Kernel了，耗时问题也就不存在了。
但是在加速未完成之前，我们一般还是会直接使用源码来调试。此时预创建所有Kernel，可以排除掉创建Kernel的时间，使得对算法的最终运行时间估计更准确。所以，这个小技巧还是有一点用的:)。</description>
    </item>
    
    <item>
      <title>OpenCV图像取反的快速方法</title>
      <link>/post/2018/06/07/opencv%E5%9B%BE%E5%83%8F%E5%8F%96%E5%8F%8D%E7%9A%84%E5%BF%AB%E9%80%9F%E6%96%B9%E6%B3%95/</link>
      <pubDate>Thu, 07 Jun 2018 21:24:10 +0000</pubDate>
      
      <guid>/post/2018/06/07/opencv%E5%9B%BE%E5%83%8F%E5%8F%96%E5%8F%8D%E7%9A%84%E5%BF%AB%E9%80%9F%E6%96%B9%E6%B3%95/</guid>
      <description>最近在做一个基于去雾的图像亮度增强的算法IBEABFHR，其中用到了图像取反的操作。所谓图像取反，就是将RGB图像的每个像素点(r, g, b)，使用(255 - r, 255 - g, 255 - b)替换。对于灰度图像而言，则是将(g)使用(255 - g)替换。
在OpenCV中要实现此操作，最简单的方法就是遍历每个像素，用255去减，此方法比较粗暴，不再赘述。
更直接地，可以对图像(cv::Mat)整体做减法：
cv::Mat image = cv::imread(&amp;quot;rgb.jpg&amp;quot;); cv::Mat image_inverse = cv::Scalar(255, 255, 255) - image;  也可以使用cv::subtract，效果是一样的:
cv::Mat image = cv::imread(&amp;quot;rgb.jpg&amp;quot;); cv::Mat image_inverse; cv::subtract(cv::Scalar(255, 255, 255), image, image_inverse);  但是我在网上搜索到了更好的方法，那就是使用位运算中的取反操作(~)：
cv::Mat image = cv::imread(&amp;quot;rgb.jpg&amp;quot;); cv::Mat image_inverse = ~image;  原理是，对于一个unsigned char类型的变量c，255 - c与~c是相等的。
此方法在opencv2\core\mat.hpp中声明如下：
CV_EXPORTS MatExpr operator ~(const Mat&amp;amp; m);  需要注意的是，此方法仅对整数型的cv::Mat有效。
一般来说，位运算的速度都是比较快的，事实也是如此，我使用一张4160x2340的图像来做测试，两种方法各运算100次取平均时间，结果如下:
 相减法：14.1862ms 位运算法：11.8158ms  </description>
    </item>
    
    <item>
      <title>提取图像细节的两种方法</title>
      <link>/post/2018/04/29/%E6%8F%90%E5%8F%96%E5%9B%BE%E5%83%8F%E7%BB%86%E8%8A%82%E7%9A%84%E4%B8%A4%E7%A7%8D%E6%96%B9%E6%B3%95/</link>
      <pubDate>Sun, 29 Apr 2018 21:50:14 +0000</pubDate>
      
      <guid>/post/2018/04/29/%E6%8F%90%E5%8F%96%E5%9B%BE%E5%83%8F%E7%BB%86%E8%8A%82%E7%9A%84%E4%B8%A4%E7%A7%8D%E6%96%B9%E6%B3%95/</guid>
      <description>一幅图像可以分解为两层：底层(base layer)和细节层(detail layer)。底层包含图像的低频信息，反映了图像在大尺度上的强度变化；细节层包含图像的高频信息，反映了图像在小尺度上的细节。分解图像有两种方式，以下分别进行解释。
1. 加性分解 要获取图像的底层，即图像的低频信息，使用低通滤波(如均值滤波(mean filter)、高斯滤波(gaussian filter)、导向滤波(guided filter)等)对图像进行滤波即可： $$B = f(I) $$ 其中\(I\)表示要分解的图像，\(f(\cdot)\)表示低通滤波操作，\(B\)为提取的底层。
提取底层后，使用源图像减去底层，即为细节层： $$D = I - B$$ 其中\(D\)表示提取的细节层。
因为底层加上细节层即为源图像，所以我称此种分解方法为加性分解，对应于加性噪声。关于此种方法的应用，可以参见[1]。
2. 乘性分解 获取底层的方法与加性分解相同。然后使用源图像除以底层，即可得到细节层： $$D = \frac{I + \epsilon}{B + \epsilon}$$ 其中\(\epsilon\)为一个很小的常数，以防止除零错误。
因为底层乘以细节层即为源图像，所以我称此种分解方法为乘性分解，对应于乘性噪声。关于此种方法的应用，可以参见[2]。在其他文章中，此处得到的细节层也称为商图像(quotient image)[3]或比例图像(ratio image)[4]。
3. 代码及效果 // 图像细节提取。 // 编程环境：Visual Studio Community 2015 + OpenCV 3.3.0 #include &amp;quot;opencv2/core/core.hpp&amp;quot; #include &amp;quot;opencv2/imgcodecs/imgcodecs.hpp&amp;quot; #include &amp;quot;opencv2/imgproc/imgproc.hpp&amp;quot; #include &amp;quot;opencv2/highgui/highgui.hpp&amp;quot; int main() { cv::Mat I = cv::imread(&amp;quot;im.png&amp;quot;); if (I.empty()) { return -1; } I.</description>
    </item>
    
  </channel>
</rss>